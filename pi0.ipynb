{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takuzen/anaconda3/envs/lerobot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-14 05:15:09.538384: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-14 05:15:09.781913: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739538909.881029  213996 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739538909.913509  213996 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-14 05:15:10.159256: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# 清空 Notebook 默认的参数，然后添加你需要的参数\n",
    "sys.argv = [\"\", \"--policy.path=lerobot/pi0\", \"--dataset.repo_id=lerobot/aloha_sim_transfer_cube_human\"]\n",
    "\n",
    "import draccus\n",
    "from lerobot.configs.train import TrainPipelineConfig, PreTrainedConfig\n",
    "from lerobot.configs import parser\n",
    "from lerobot.common.datasets.transforms import ImageTransforms\n",
    "from lerobot.common.datasets.lerobot_dataset import (\n",
    "    LeRobotDataset,\n",
    "    LeRobotDatasetMetadata,\n",
    "    MultiLeRobotDataset,\n",
    ")\n",
    "import math\n",
    "from lerobot.common.utils.utils import get_safe_dtype\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    GemmaForCausalLM\n",
    ")\n",
    "from transformers.models.auto import CONFIG_MAPPING\n",
    "\n",
    "from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.configs.types import FeatureType\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from lerobot.common.datasets.utils import cycle\n",
    "from lerobot.common.policies.normalize import Normalize, Unnormalize\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "\n",
    "\n",
    "def resolve_delta_timestamps(\n",
    "    cfg: PreTrainedConfig, ds_meta: LeRobotDatasetMetadata\n",
    ") -> dict[str, list] | None:\n",
    "    \"\"\"Resolves delta_timestamps by reading from the 'delta_indices' properties of the PreTrainedConfig.\n",
    "\n",
    "    Args:\n",
    "        cfg (PreTrainedConfig): The PreTrainedConfig to read delta_indices from.\n",
    "        ds_meta (LeRobotDatasetMetadata): The dataset from which features and fps are used to build\n",
    "            delta_timestamps against.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list] | None: A dictionary of delta_timestamps, e.g.:\n",
    "            {\n",
    "                \"observation.state\": [-0.04, -0.02, 0]\n",
    "                \"observation.action\": [-0.02, 0, 0.02]\n",
    "            }\n",
    "            returns `None` if the the resulting dict is empty.\n",
    "    \"\"\"\n",
    "    delta_timestamps = {}\n",
    "    for key in ds_meta.features:\n",
    "        if key == \"next.reward\" and cfg.reward_delta_indices is not None:\n",
    "            delta_timestamps[key] = [i / ds_meta.fps for i in cfg.reward_delta_indices]\n",
    "        if key == \"action\" and cfg.action_delta_indices is not None:\n",
    "            delta_timestamps[key] = [i / ds_meta.fps for i in cfg.action_delta_indices]\n",
    "        if key.startswith(\"observation.\") and cfg.observation_delta_indices is not None:\n",
    "            delta_timestamps[key] = [i / ds_meta.fps for i in cfg.observation_delta_indices]\n",
    "\n",
    "    if len(delta_timestamps) == 0:\n",
    "        delta_timestamps = None\n",
    "\n",
    "    return delta_timestamps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaWithExpertConfig(PretrainedConfig):\n",
    "    model_type = \"PaliGemmaWithExpertModel\"\n",
    "    sub_configs = {\"paligemma_config\": AutoConfig, \"gemma_expert_config\": AutoConfig}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        paligemma_config: dict | None = None,\n",
    "        gemma_expert_config: dict | None = None,\n",
    "        freeze_vision_encoder: bool = True,\n",
    "        train_expert_only: bool = True,\n",
    "        attention_implementation: str = \"eager\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.freeze_vision_encoder = freeze_vision_encoder\n",
    "        self.train_expert_only = train_expert_only\n",
    "        self.attention_implementation = attention_implementation\n",
    "\n",
    "        if paligemma_config is None:\n",
    "            # Default config from Pi0\n",
    "            self.paligemma_config = CONFIG_MAPPING[\"paligemma\"](\n",
    "                transformers_version=\"4.48.1\",\n",
    "                _vocab_size=257152,\n",
    "                bos_token_id=2,\n",
    "                eos_token_id=1,\n",
    "                hidden_size=2048,\n",
    "                image_token_index=257152,\n",
    "                model_type=\"paligemma\",\n",
    "                pad_token_id=0,\n",
    "                projection_dim=2048,\n",
    "                text_config={\n",
    "                    \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
    "                    \"hidden_size\": 2048,\n",
    "                    \"intermediate_size\": 16384,\n",
    "                    \"model_type\": \"gemma\",\n",
    "                    \"num_attention_heads\": 8,\n",
    "                    \"num_hidden_layers\": 18,\n",
    "                    \"num_image_tokens\": 256,\n",
    "                    \"num_key_value_heads\": 1,\n",
    "                    \"torch_dtype\": \"float32\",\n",
    "                    \"vocab_size\": 257152,\n",
    "                },\n",
    "                vision_config={\n",
    "                    \"hidden_size\": 1152,\n",
    "                    \"intermediate_size\": 4304,\n",
    "                    \"model_type\": \"siglip_vision_model\",\n",
    "                    \"num_attention_heads\": 16,\n",
    "                    \"num_hidden_layers\": 27,\n",
    "                    \"num_image_tokens\": 256,\n",
    "                    \"patch_size\": 14,\n",
    "                    \"projection_dim\": 2048,\n",
    "                    \"projector_hidden_act\": \"gelu_fast\",\n",
    "                    \"torch_dtype\": \"float32\",\n",
    "                    \"vision_use_head\": False,\n",
    "                },\n",
    "            )\n",
    "        elif isinstance(self.paligemma_config, dict):\n",
    "            # Override Pi0 default config for PaliGemma\n",
    "            if \"model_type\" not in gemma_expert_config:\n",
    "                paligemma_config[\"model_type\"] = \"paligemma\"\n",
    "\n",
    "            cfg_cls = CONFIG_MAPPING[paligemma_config[\"model_type\"]]\n",
    "            self.paligemma_config = cfg_cls(**paligemma_config)\n",
    "\n",
    "        if gemma_expert_config is None:\n",
    "            # Default config from Pi0\n",
    "            self.gemma_expert_config = CONFIG_MAPPING[\"gemma\"](\n",
    "                attention_bias=False,\n",
    "                attention_dropout=0.0,\n",
    "                bos_token_id=2,\n",
    "                eos_token_id=1,\n",
    "                head_dim=256,\n",
    "                hidden_act=\"gelu_pytorch_tanh\",\n",
    "                hidden_activation=\"gelu_pytorch_tanh\",\n",
    "                hidden_size=1024,\n",
    "                initializer_range=0.02,\n",
    "                intermediate_size=4096,\n",
    "                max_position_embeddings=8192,\n",
    "                model_type=\"gemma\",\n",
    "                num_attention_heads=8,\n",
    "                num_hidden_layers=18,\n",
    "                num_key_value_heads=1,\n",
    "                pad_token_id=0,\n",
    "                rms_norm_eps=1e-06,\n",
    "                rope_theta=10000.0,\n",
    "                torch_dtype=\"float32\",\n",
    "                transformers_version=\"4.48.1\",\n",
    "                use_cache=True,\n",
    "                vocab_size=257152,\n",
    "            )\n",
    "        elif isinstance(self.gemma_expert_config, dict):\n",
    "            # Override Pi0 default config for Gemma Expert\n",
    "            if \"model_type\" not in gemma_expert_config:\n",
    "                gemma_expert_config[\"model_type\"] = \"gemma\"\n",
    "\n",
    "            cfg_cls = CONFIG_MAPPING[paligemma_config[\"model_type\"]]\n",
    "            self.gemma_expert_config = cfg_cls(**gemma_expert_config)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        if self.train_expert_only and not self.freeze_vision_encoder:\n",
    "            raise ValueError(\n",
    "                \"You set `freeze_vision_encoder=False` and `train_expert_only=True` which are not compatible.\"\n",
    "            )\n",
    "\n",
    "        if self.attention_implementation not in [\"eager\", \"fa2\", \"flex\"]:\n",
    "            raise ValueError(\n",
    "                f\"Wrong value provided for `attention_implementation` ({self.attention_implementation}). Expected 'eager', 'fa2' or 'flex'.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x, positions, max_wavelength=20_000):\n",
    "    print(\"apply_rope\")\n",
    "    print(f\"query states shape: {x.shape}\")\n",
    "    dtype = x.dtype\n",
    "    d_half = x.shape[-1] // 2\n",
    "    print(f\"d_half: {d_half}\")\n",
    "    x = x.to(torch.float32)\n",
    "    print(f\"x shape: {x.shape}\")\n",
    "    freq_exponents = (2 / x.shape[-1]) * torch.arange(d_half, device=x.device, dtype=x.dtype)\n",
    "    print(f\"freq_exponents shape: {freq_exponents.shape}\")\n",
    "    timescale = max_wavelength**freq_exponents\n",
    "    print(f\"timescale shape: {timescale.shape}\")\n",
    "    print(f\"positions shape: {positions.shape}\")\n",
    "    print(f\"positions[..., None] shape: {positions[..., None].shape}\")\n",
    "    print(f\"timescale[None, None, :] shape: {timescale[None, None, :].shape}\")\n",
    "    radians = positions[..., None].to(torch.float32) * timescale[None, None, :]\n",
    "    print(f\"radians shape: {radians.shape}\")\n",
    "    #x shape: torch.Size([1, 355, 8, 256])\n",
    "    radians = radians[..., None, :]\n",
    "\n",
    "    sin = torch.sin(radians)\n",
    "    cos = torch.cos(radians)\n",
    "\n",
    "    x1, x2 = x.split(d_half, dim=-1)\n",
    "    res = torch.empty_like(x)\n",
    "    res[..., :d_half] = x1 * cos - x2 * sin\n",
    "    res[..., d_half:] = x1 * sin + x2 * cos\n",
    "\n",
    "    return res.to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaWithExpertModel(PreTrainedModel):\n",
    "    config_class = PaliGemmaWithExpertConfig\n",
    "\n",
    "    def __init__(self, config: PaliGemmaWithExpertConfig):\n",
    "        super().__init__(config=config)\n",
    "        self.config = config\n",
    "        self.paligemma = PaliGemmaForConditionalGeneration(config=config.paligemma_config)\n",
    "        self.gemma_expert = GemmaForCausalLM(config=config.gemma_expert_config)\n",
    "        self.paligemma = self.paligemma.to(dtype=torch.bfloat16)\n",
    "        params_to_change_dtype = [\n",
    "            \"language_model.model.layers\",\n",
    "            \"gemma_expert.model.layers\",\n",
    "            \"vision_tower\",\n",
    "            \"multi_modal\",\n",
    "        ]\n",
    "        for name, param in self.named_parameters():\n",
    "            if any(selector in name for selector in params_to_change_dtype):\n",
    "                param.data = param.data.to(dtype=torch.bfloat16)\n",
    "\n",
    "    def embed_image(self, image):\n",
    "        return self.paligemma.get_image_features(image)\n",
    "    \n",
    "    def embed_lang_tokens(self, lang_tokens):\n",
    "        return self.paligemma.language_model.model.embed_tokens(lang_tokens)\n",
    "    \n",
    "    def forward(self, attention_mask, position_ids, inputs_embeds):\n",
    "        models = [self.paligemma.language_model.model, self.gemma_expert.model]\n",
    "        \n",
    "        num_layers = self.paligemma.config.text_config.num_hidden_layers\n",
    "        head_dim = self.paligemma.config.text_config.head_dim\n",
    "        batch_size = inputs_embeds[0].shape[0]\n",
    "        for layer_idx in range(num_layers):\n",
    "            query_states = []\n",
    "            key_states = []\n",
    "            value_states = []\n",
    "            for i, hidden_states in enumerate(inputs_embeds):\n",
    "                layer = models[i].layers[layer_idx]\n",
    "                print(f\"hideen_states.shape: {hidden_states.shape}\")\n",
    "                hidden_states = layer.input_layernorm(hidden_states)\n",
    "                print(f\"hideen_states.shape: {hidden_states.shape}\")\n",
    "\n",
    "                # hideen_states.shape: torch.Size([1, 304, 2048]) prefix\n",
    "                # hideen_states.shape: torch.Size([1, 51, 1024]) suffix\n",
    "                input_shape = hidden_states.shape[:-1]\n",
    "                hidden_shape = (*input_shape, -1, layer.self_attn.head_dim)\n",
    "\n",
    "                hidden_states = hidden_states.to(dtype=torch.bfloat16)\n",
    "\n",
    "                print(f\"hideen_states.shape: {hidden_states.shape}\")\n",
    "\n",
    "                query_state = layer.self_attn.q_proj(hidden_states).view(hidden_shape)\n",
    "                key_state = layer.self_attn.k_proj(hidden_states).view(hidden_shape)\n",
    "                value_state = layer.self_attn.v_proj(hidden_states).view(hidden_shape)\n",
    "\n",
    "                print(f\"query_state.shape: {query_state.shape}\")\n",
    "                print(f\"key_state.shape: {key_state.shape}\")\n",
    "                print(f\"value_state.shape: {value_state.shape}\")\n",
    "\n",
    "                query_states.append(query_state)\n",
    "                key_states.append(key_state)\n",
    "                value_states.append(value_state)\n",
    "\n",
    "            query_states = torch.cat(query_states, dim=1)\n",
    "            key_states = torch.cat(key_states, dim=1)\n",
    "            value_states = torch.cat(value_states, dim=1)\n",
    "\n",
    "            query_states = apply_rope(query_states, position_ids)\n",
    "            print(f\"after rope query_states.shape: {query_states.shape}\")\n",
    "            key_states = apply_rope(key_states, position_ids)\n",
    "            \n",
    "            att_output = self.eager_attention_forward(attention_mask, batch_size, head_dim, query_states, key_states, value_states)\n",
    "            print(f\"att_output after qkv: {att_output.shape}\")\n",
    "\n",
    "            att_output = att_output.to(dtype=torch.bfloat16)\n",
    "\n",
    "            outputs_embeds = []\n",
    "            start = 0\n",
    "            for i, hidden_states in enumerate(inputs_embeds):\n",
    "                layer = models[i].layers[layer_idx]\n",
    "                if hidden_states is not None:\n",
    "                    end = start + hidden_states.shape[1]\n",
    "                    out_emb = layer.self_attn.o_proj(att_output[:, start:end])\n",
    "                    out_emb += hidden_states\n",
    "                    after_first_residual = out_emb.clone()\n",
    "\n",
    "                    out_emb = layer.post_attention_layernorm(out_emb)\n",
    "\n",
    "                    out_emb = layer.mlp(out_emb)\n",
    "\n",
    "                    out_emb += after_first_residual\n",
    "\n",
    "                    outputs_embeds.append(out_emb)\n",
    "                    start = end\n",
    "                else:\n",
    "                    outputs_embeds.append(None)\n",
    "\n",
    "            inputs_embeds = outputs_embeds\n",
    "        \n",
    "        outputs_embeds = []\n",
    "        for i, hidden_states in enumerate(inputs_embeds):\n",
    "            if hidden_states is not None:\n",
    "                out_emb = models[i].norm(hidden_states)\n",
    "                outputs_embeds.append(out_emb)\n",
    "            else:\n",
    "                outputs_embeds.append(None)\n",
    "        \n",
    "        return outputs_embeds\n",
    "\n",
    "\n",
    "    def eager_attention_forward(\n",
    "        self, attention_mask, batch_size, head_dim, query_states, key_states, value_states\n",
    "    ):\n",
    "        num_attn_heads = self.paligemma.config.text_config.num_attention_heads\n",
    "        num_key_value_heads = self.paligemma.config.text_config.num_key_value_heads\n",
    "        print(f\"num_attn_heads: {num_attn_heads}\")\n",
    "        print(f\"num_key_value_heads: {num_key_value_heads}\")\n",
    "        num_key_value_groups = num_attn_heads // num_key_value_heads\n",
    "        print(f\"num_key_value_groups: {num_key_value_groups}\")\n",
    "\n",
    "        sequence_length = key_states.shape[1]\n",
    "        print(f\"sequence_length: {sequence_length}\")\n",
    "\n",
    "        print(f\"value_states.shape: {value_states.shape}\")\n",
    "\n",
    "        # key_states.shape: torch.Size([1, 304, 1, 256])\n",
    "        key_states = key_states[:, :, :, None, :].expand(\n",
    "            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim\n",
    "        )\n",
    "\n",
    "        key_states = key_states.reshape(\n",
    "            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim\n",
    "        )\n",
    "\n",
    "        value_states = value_states[:, :, :, None, :].expand(\n",
    "            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim\n",
    "        )\n",
    "\n",
    "        value_states = value_states.reshape(\n",
    "            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim\n",
    "        )\n",
    "\n",
    "        print(f\"key_states.shape: {key_states.shape}\")\n",
    "        print(f\"query_states.shape: {query_states.shape}\")\n",
    "\n",
    "        query_states = query_states.to(dtype=torch.float32)\n",
    "        key_states = key_states.to(dtype=torch.float32)\n",
    "\n",
    "        query_states = query_states.transpose(1, 2)\n",
    "        key_states = key_states.transpose(1, 2)\n",
    "\n",
    "        # key_states.shape: torch.Size([355, 1, 8, 256])\n",
    "        # query_states.shape: torch.Size([355, 1, 8, 256])\n",
    "\n",
    "        att_weights = torch.matmul(query_states, key_states.transpose(2, 3))\n",
    "        att_weights *= head_dim ** -0.5\n",
    "\n",
    "        # att_weights.shape: torch.Size([1, 8, 355, 355])\n",
    "        # attention_mask.shape: torch.Size([1, 355, 355])\n",
    "        big_neg = -2.3819763e38\n",
    "        att_weights = torch.where(attention_mask[:, None, :, :], att_weights, big_neg)\n",
    "        print(f\"att_weights.shape: {att_weights.shape}\")\n",
    "        probs = nn.functional.softmax(att_weights, dim=-1)\n",
    "        print(f\"probs.shape: {probs.shape}\")\n",
    "\n",
    "        probs = probs.to(dtype=torch.bfloat16)\n",
    "        print(f\"value_states.shape: {value_states.shape}\")\n",
    "        attn_output = torch.matmul(probs, value_states.permute(0, 2, 1, 3))\n",
    "        print(f\"attn_output.shape: {attn_output.shape}\")\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(batch_size, -1, num_key_value_heads * num_key_value_groups * head_dim)\n",
    "        print(f\"attn_output.shape: {attn_output.shape}\")\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi0(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        paligemma_with_export_config = PaliGemmaWithExpertConfig(\n",
    "            freeze_vision_encoder=cfg.policy.freeze_vision_encoder,\n",
    "            train_expert_only=cfg.policy.train_expert_only,\n",
    "            attention_implementation=cfg.policy.attention_implementation,\n",
    "        )\n",
    "        self.paligemma_with_expert = PaliGemmaWithExpertModel(paligemma_with_export_config)\n",
    "        self.state_proj = nn.Linear(cfg.policy.max_state_dim, cfg.policy.proj_width).to(torch.bfloat16)\n",
    "        self.action_proj = nn.Linear(cfg.policy.max_action_dim, cfg.policy.proj_width).to(torch.bfloat16)\n",
    "\n",
    "        self.action_time_mlp_in = nn.Linear(cfg.policy.proj_width * 2, cfg.policy.proj_width).to(torch.bfloat16)\n",
    "        self.action_time_mlp_out = nn.Linear(cfg.policy.proj_width, cfg.policy.proj_width).to(torch.bfloat16)\n",
    "        self.action_out_proj = nn.Linear(cfg.policy.proj_width, cfg.policy.max_action_dim).to(torch.bfloat16)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def create_sinusoidal_pos_embedding(self, \n",
    "        time: torch.tensor, dimension: int, min_period: float, max_period: float, device=\"cpu\"):\n",
    "        \"\"\"Computes sine-cosine positional embedding vectors for scalar positions.\"\"\"\n",
    "        if dimension % 2 != 0:\n",
    "            raise ValueError(f\"dimension ({dimension}) must be divisible by 2\")\n",
    "\n",
    "        if time.ndim != 1:\n",
    "            raise ValueError(\"The time tensor is expected to be of shape `(batch_size, )`.\")\n",
    "\n",
    "        dtype = get_safe_dtype(torch.float64, device.type)\n",
    "        fraction = torch.linspace(0.0, 1.0, dimension // 2, dtype=dtype, device=device)\n",
    "        period = min_period * (max_period / min_period) ** fraction\n",
    "\n",
    "        # Compute the outer product\n",
    "        scaling_factor = 1.0 / period * 2 * math.pi\n",
    "        sin_input = scaling_factor[None, :] * time[:, None]\n",
    "        pos_emb = torch.cat([torch.sin(sin_input), torch.cos(sin_input)], dim=1)\n",
    "        return pos_emb\n",
    "\n",
    "    def embed_prefix(self, images, img_masks, lang_tokens, lang_masks):\n",
    "        embs = []\n",
    "        pad_maks = []\n",
    "        att_masks = []\n",
    "\n",
    "        for (img, img_mask) in zip(images, img_masks, strict=True):\n",
    "            img_emb = self.paligemma_with_expert.embed_image(img)\n",
    "            print(\"img_emb\", img_emb.shape)\n",
    "            img_emb = img_emb.to(dtype=torch.bfloat16)\n",
    "\n",
    "            # img_emb torch.Size([1, 256, 2048])\n",
    "            # Normalize image embeddings\n",
    "            img_emb_dim = img_emb.shape[-1]\n",
    "            img_emb = img_emb * torch.tensor(img_emb_dim**0.5, dtype=img_emb.dtype, device=img_emb.device)\n",
    "\n",
    "            print(\"img_mask\", img_mask.shape)\n",
    "\n",
    "            b_size, num_img_embds = img_emb.shape[:2]\n",
    "            img_mask = img_mask[:, None].expand(b_size, num_img_embds)\n",
    "            print(\"img_mask\", img_mask.shape)\n",
    "\n",
    "            embs.append(img_emb)\n",
    "            pad_maks.append(img_mask)\n",
    "\n",
    "            att_masks += [0] * num_img_embds\n",
    "        \n",
    "        lang_emb = self.paligemma_with_expert.embed_lang_tokens(lang_tokens)\n",
    "\n",
    "        print(\"lang_emb\", lang_emb.shape)\n",
    "\n",
    "        lang_emb_dim = lang_emb.shape[-1]\n",
    "        lang_emb = lang_emb * math.sqrt(lang_emb_dim)\n",
    "        \n",
    "        print(\"lang_emb_dim\", lang_emb_dim)\n",
    "        print(\"lang_masks\", lang_masks.shape)\n",
    "\n",
    "        embs.append(lang_emb)\n",
    "        pad_maks.append(lang_masks)\n",
    "\n",
    "        att_masks += [0] * lang_emb.shape[1]\n",
    "\n",
    "        embs = torch.cat(embs, dim=1)\n",
    "        pad_maks = torch.cat(pad_maks, dim=1)\n",
    "\n",
    "        attn_masks = torch.tensor(att_masks, dtype=torch.bool, device=pad_maks.device)\n",
    "        att_masks = attn_masks[None, :].expand(b_size, len(att_masks))\n",
    "\n",
    "        return embs, pad_maks, att_masks\n",
    "\n",
    "    def emb_suffix(self, states, noisy_actions, timestep):\n",
    "        embs = []\n",
    "        pad_maks = []\n",
    "        att_masks = []\n",
    "\n",
    "        state_emb = self.state_proj(states)\n",
    "        print(f'state_emb {state_emb.shape}')\n",
    "        state_emb = state_emb.to(dtype=torch.bfloat16)\n",
    "        embs.append(state_emb[:, None, :])\n",
    "\n",
    "        state_mask = torch.ones((states.shape[0], 1), dtype=torch.bool, device=state_emb.device)\n",
    "        pad_maks.append(state_mask)\n",
    "\n",
    "        att_masks += [1]\n",
    "\n",
    "        time_emb = self.create_sinusoidal_pos_embedding(timestep, self.cfg.policy.proj_width, min_period=0.001, max_period=4.0, device=state_emb.device) \n",
    "        time_emb = time_emb.to(dtype=state_emb.dtype)\n",
    "\n",
    "        action_emb = self.action_proj(noisy_actions)\n",
    "        print(f'action_emb {action_emb.shape}')\n",
    "        print(f'time_emb {time_emb.shape}')\n",
    "\n",
    "        time_emb = time_emb[:, None, :].expand(-1, action_emb.shape[1], -1)\n",
    "        action_time_emb = torch.cat([action_emb, time_emb], dim=-1)\n",
    "        print(f'action_time_emb {action_time_emb.shape}')\n",
    "\n",
    "        action_time_emb = self.action_time_mlp_in(action_time_emb)\n",
    "        action_time_emb = F.silu(action_time_emb)\n",
    "        action_time_emb = self.action_time_mlp_out(action_time_emb)\n",
    "\n",
    "        embs.append(action_time_emb)\n",
    "\n",
    "        action_mask = torch.ones(action_time_emb.shape[0], action_time_emb.shape[1], dtype=torch.bool, device=action_time_emb.device)\n",
    "        pad_maks.append(action_mask)\n",
    "\n",
    "        att_masks += [1] + ([0] * (self.cfg.policy.n_action_steps - 1))\n",
    "\n",
    "        print(f\"embs len {len(embs)}\")\n",
    "        for i, emb in enumerate(embs):\n",
    "            print(f\"Embedding {i} shape: {emb.shape}\")\n",
    "\n",
    "        print(f\"action_mask len {len(pad_maks)}\")\n",
    "        for i, emb in enumerate(pad_maks):\n",
    "            print(f\"action_mask {i} shape: {emb.shape}\")\n",
    "\n",
    "        att_masks = torch.tensor(att_masks, dtype=action_time_emb.dtype, device=action_time_emb.device)\n",
    "\n",
    "        print(f\"att_masks shape {att_masks.shape}\")\n",
    "        \n",
    "        embs = torch.cat(embs, dim=1)\n",
    "        pad_maks = torch.cat(pad_maks, dim=1)\n",
    "        att_masks = att_masks[None, :].expand(embs.shape[0], len(att_masks))\n",
    "\n",
    "        return embs, pad_maks, att_masks\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, images, img_masks, lang_tokens, lang_masks, states, actions):\n",
    "        noise = torch.normal(\n",
    "            mean=0.0, std=1.0, size=actions.shape,\n",
    "            dtype=torch.bfloat16, device=actions.device,\n",
    "        )\n",
    "        gamma1 = torch.empty((actions.shape[0],), device=actions.device).uniform_(0, 1).pow(1 / 1.5)\n",
    "        gamma2 = torch.empty((actions.shape[0],), device=actions.device).uniform_(0, 1).pow(1 / 1.0)  \n",
    "        time_beta = gamma1 / (gamma1 + gamma2)\n",
    "        time = time_beta * 0.999 + 0.001\n",
    "        time = time.to(dtype=torch.bfloat16, device=actions.device)\n",
    "\n",
    "        time_expanded = time[:, None, None]\n",
    "        print(time_expanded.shape)\n",
    "        print(noise.shape)\n",
    "        print(actions.shape)\n",
    "        x_t = time_expanded * noise + (1 - time_expanded) * actions\n",
    "        u_t = noise - actions\n",
    "\n",
    "        prefix_embs, prefix_pad_masks, prefix_att_masks =  self.embed_prefix(\n",
    "            images, img_masks, lang_tokens, lang_masks\n",
    "        )\n",
    "\n",
    "        suffix_embs, suffix_pad_masks, suffix_att_masks =  self.emb_suffix(states, x_t, time)\n",
    "\n",
    "\n",
    "        print(f\"prefix_embs {prefix_embs.shape}\")\n",
    "        print(f\"prefix_pad_masks {prefix_pad_masks.shape}\")\n",
    "        print(f\"prefix_att_masks {prefix_att_masks.shape}\")\n",
    "        print(f\"suffix_embs {suffix_embs.shape}\")\n",
    "        print(f\"suffix_pad_masks {suffix_pad_masks.shape}\")\n",
    "        print(f\"suffix_att_masks {suffix_att_masks.shape}\")\n",
    "\n",
    "        pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=1)\n",
    "        att_masks = torch.cat([prefix_att_masks, suffix_att_masks], dim=1)\n",
    "\n",
    "        cumsum = torch.cumsum(att_masks, dim=1)\n",
    "        att_2d_masks = cumsum[:, None, :] <= cumsum[:, :, None]\n",
    "        pad_2d_masks = pad_masks[:, None, :] * pad_masks[:, :, None]\n",
    "        att_2d_masks = att_2d_masks * pad_2d_masks\n",
    "        \n",
    "        position_ids = torch.cumsum(pad_masks, dim=1) - 1\n",
    "        \n",
    "        (_, outputs_embeds) = self.paligemma_with_expert.forward(\n",
    "            attention_mask=att_2d_masks,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=[prefix_embs, suffix_embs],\n",
    "        )\n",
    "\n",
    "        #see the shape of output embeds\n",
    "        for i, emb in enumerate(outputs_embeds):\n",
    "            print(f\"Output Embedding {i} shape: {emb.shape}\")\n",
    "\n",
    "        suffix_out = outputs_embeds[:,-self.cfg.policy.n_action_steps:]\n",
    "        print(f\"suffix_out shape: {suffix_out.shape}\")\n",
    "\n",
    "        suffix_out = suffix_out.to(dtype=torch.bfloat16)\n",
    "        v_t = self.action_out_proj(suffix_out)\n",
    "        print(f\"v_t shape: {v_t.shape}\")\n",
    "\n",
    "        losses = F.mse_loss(v_t, u_t, reduction=\"none\")\n",
    "        print(f\"losses: {losses.mean()}\")\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRunner(nn.Module):\n",
    "    def __init__(self, cfg, ds_meta):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.normalize_inputs = Normalize(cfg.input_features, cfg.policy.normalization_mapping, ds_meta.stats)\n",
    "        self.normalize_targets = Normalize(\n",
    "            cfg.output_features, cfg.policy.normalization_mapping, ds_meta.stats\n",
    "        )\n",
    "        self.unnormalize_outputs = Unnormalize(\n",
    "            cfg.output_features, cfg.policy.normalization_mapping, ds_meta.stats\n",
    "        )\n",
    "        self.language_tokenizer = AutoTokenizer.from_pretrained(\"google/paligemma-3b-pt-224\")\n",
    "        self.model = Pi0(cfg)\n",
    "\n",
    "    def resize_with_pad(sefl, img, width, height, pad_value=-1):\n",
    "        # assume no-op when width height fits already\n",
    "        if img.ndim != 4:\n",
    "            raise ValueError(f\"(b,c,h,w) expected, but {img.shape}\")\n",
    "\n",
    "        cur_height, cur_width = img.shape[2:]\n",
    "        #img shape: torch.Size([1, 3, 480, 640])\n",
    "  \n",
    "        ratio = max(cur_width / width, cur_height / height)\n",
    "        resized_height = int(cur_height / ratio)\n",
    "        resized_width = int(cur_width / ratio)\n",
    "        resized_img = F.interpolate(\n",
    "            img, size=(resized_height, resized_width), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "        pad_height = max(0, int(height - resized_height))\n",
    "        pad_width = max(0, int(width - resized_width))\n",
    "        \n",
    "\n",
    "        # pad on left and top of image\n",
    "        padded_img = F.pad(resized_img, (pad_width, 0, pad_height, 0), value=pad_value)\n",
    "\n",
    "        return padded_img\n",
    "\n",
    "    def prepare_image(self, batch):\n",
    "        images = []\n",
    "        img_masks = []\n",
    "        present_img_keys = [key for key in self.cfg.image_features if key in batch]\n",
    "\n",
    "        for key in present_img_keys:\n",
    "            print(f\"processing img key: {key}\")\n",
    "            img = batch[key]\n",
    "            if self.cfg.policy.resize_imgs_with_padding is not None:\n",
    "                img = self.resize_with_pad(img, *self.cfg.policy.resize_imgs_with_padding, pad_value=0)\n",
    "                print(f\"img shape: {img.shape}\")\n",
    "\n",
    "            print(f\"img: {img}\")\n",
    "            img = img * 2.0 - 1.0\n",
    "            print(f\"img: {img}\")\n",
    "\n",
    "            mask = torch.ones(img.shape[0], dtype=torch.bool, device=img.device)\n",
    "            images.append(img)\n",
    "            img_masks.append(mask)\n",
    "\n",
    "        return images, img_masks\n",
    "\n",
    "    def prepare_language(self, batch):\n",
    "        tasks = batch[\"task\"]\n",
    "        device = batch[\"observation.state\"].device\n",
    "        print(tasks)\n",
    "        tasks = [task if task.endswith(\"\\n\") else f\"{task}\\n\" for task in tasks]\n",
    "        print(tasks)\n",
    "        print(self.cfg.policy.tokenizer_max_length)\n",
    "\n",
    "        tokenized_prompt = self.language_tokenizer.__call__(\n",
    "            tasks,\n",
    "            padding=\"max_length\",\n",
    "            padding_side=\"right\",\n",
    "            max_length=self.cfg.policy.tokenizer_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        lang_tokens = tokenized_prompt[\"input_ids\"].to(device=device)\n",
    "        lang_masks = tokenized_prompt[\"attention_mask\"].to(device=device, dtype=torch.bool)\n",
    "\n",
    "        return lang_tokens, lang_masks\n",
    "    \n",
    "    def pad_vector(self, vector, new_dim):\n",
    "        shape = list(vector.shape)\n",
    "        curren_dim = shape[-1]\n",
    "        shape[-1] = new_dim\n",
    "        new_vector = torch.zeros(*shape, dtype=vector.dtype, device=vector.device)\n",
    "        new_vector[..., :curren_dim] = vector\n",
    "        # paded_vector = torch.zeros(vector.shape[0], new_dim, device=vector.device)\n",
    "        # paded_vector[:, :vector.shape[1]] = vector\n",
    "        return new_vector\n",
    "\n",
    "\n",
    "    def prepare_action(self, batch):\n",
    "        actions = self.pad_vector(batch[\"action\"], self.cfg.policy.max_action_dim)\n",
    "        return actions\n",
    "    \n",
    "    def prepare_state(self, batch):\n",
    "        states = self.pad_vector(batch[\"observation.state\"], self.cfg.policy.max_state_dim)\n",
    "        return states\n",
    "\n",
    "    def forward(self, batch):\n",
    "        print(f\"before normalize: {batch}\")\n",
    "        batch = self.normalize_inputs(batch)\n",
    "        batch = self.normalize_targets(batch)\n",
    "        print(f\"after normalize: {batch}\")\n",
    "\n",
    "        images, img_masks = self.prepare_image(batch)\n",
    "        lang_tokens, lang_masks = self.prepare_language(batch)\n",
    "        actions = self.prepare_action(batch)\n",
    "        states = self.prepare_state(batch)\n",
    "        \n",
    "        for img in images:\n",
    "            img = img.to(torch.bfloat16)\n",
    "        for img_mask in img_masks:\n",
    "            img_mask = img_mask.to(torch.bfloat16)\n",
    "        for lang_tk in lang_tokens:\n",
    "            lang_tk = lang_tk.to(torch.bfloat16)\n",
    "        for lang_mask in lang_masks:\n",
    "            lang_mask = lang_mask.to(torch.bfloat16)\n",
    "        actions = actions.to(torch.bfloat16)        \n",
    "        states = states.to(torch.bfloat16)\n",
    "\n",
    "        self.model.forward(images, img_masks, lang_tokens, lang_masks, states, actions)\n",
    "\n",
    "        return batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argspec is FullArgSpec(args=['cfg'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={'cfg': <class 'lerobot.configs.train.TrainPipelineConfig'>})\n",
      "argtype is <class 'lerobot.configs.train.TrainPipelineConfig'>\n",
      "cli_args is ['--policy.path=lerobot/pi0', '--dataset.repo_id=lerobot/aloha_sim_transfer_cube_human']\n",
      "config_path_cli is None\n",
      "path_fields is ['policy']\n",
      "cli_args is ['--dataset.repo_id=lerobot/aloha_sim_transfer_cube_human']\n",
      "argtype is <class 'lerobot.configs.train.TrainPipelineConfig'>\n",
      "config_path is None\n",
      "cli_args is ['--dataset.repo_id=lerobot/aloha_sim_transfer_cube_human']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No device specified, trying to infer device automatically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg is TrainPipelineConfig(dataset=DatasetConfig(repo_id='lerobot/aloha_sim_transfer_cube_human', episodes=None, image_transforms=ImageTransformsConfig(enable=False, max_num_transforms=3, random_order=False, tfs={'brightness': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'brightness': (0.8, 1.2)}), 'contrast': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'contrast': (0.8, 1.2)}), 'saturation': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'saturation': (0.5, 1.5)}), 'hue': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'hue': (-0.05, 0.05)}), 'sharpness': ImageTransformConfig(weight=1.0, type='SharpnessJitter', kwargs={'sharpness': (0.5, 1.5)})}), local_files_only=False, use_imagenet_stats=True, video_backend='pyav'), env=None, policy=None, output_dir=None, job_name=None, resume=False, device=None, use_amp=False, seed=1000, num_workers=4, batch_size=4, eval_freq=20000, log_freq=200, save_checkpoint=True, save_freq=20, offline=OfflineConfig(steps=100000), online=OnlineConfig(steps=0, rollout_n_episodes=1, rollout_batch_size=1, steps_between_rollouts=None, sampling_ratio=0.5, env_seed=None, buffer_capacity=None, buffer_seed_size=0, do_rollout_async=False), use_policy_training_preset=True, optimizer=None, scheduler=None, eval=EvalConfig(n_episodes=50, batch_size=50, use_async_envs=False), wandb=WandBConfig(enable=False, disable_artifact=False, project='lerobot', entity=None, notes=None))\n",
      "Before from_pretrained call, cfg is: TrainPipelineConfig(dataset=DatasetConfig(repo_id='lerobot/aloha_sim_transfer_cube_human', episodes=None, image_transforms=ImageTransformsConfig(enable=False, max_num_transforms=3, random_order=False, tfs={'brightness': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'brightness': (0.8, 1.2)}), 'contrast': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'contrast': (0.8, 1.2)}), 'saturation': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'saturation': (0.5, 1.5)}), 'hue': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'hue': (-0.05, 0.05)}), 'sharpness': ImageTransformConfig(weight=1.0, type='SharpnessJitter', kwargs={'sharpness': (0.5, 1.5)})}), local_files_only=False, use_imagenet_stats=True, video_backend='pyav'), env=None, policy=None, output_dir=None, job_name=None, resume=False, device=None, use_amp=False, seed=1000, num_workers=4, batch_size=4, eval_freq=20000, log_freq=200, save_checkpoint=True, save_freq=20, offline=OfflineConfig(steps=100000), online=OnlineConfig(steps=0, rollout_n_episodes=1, rollout_batch_size=1, steps_between_rollouts=None, sampling_ratio=0.5, env_seed=None, buffer_capacity=None, buffer_seed_size=0, do_rollout_async=False), use_policy_training_preset=True, optimizer=None, scheduler=None, eval=EvalConfig(n_episodes=50, batch_size=50, use_async_envs=False), wandb=WandBConfig(enable=False, disable_artifact=False, project='lerobot', entity=None, notes=None))\n",
      "policy_path is lerobot/pi0\n",
      "cli_overrides is []\n",
      "pretrained_name_or_path is lerobot/pi0\n",
      "model_id is lerobot/pi0\n",
      "config_file is None\n",
      "CONFIG_NAME is config.json\n",
      "revision is None\n",
      "cache_dir is None\n",
      "token is None\n",
      "proxies is None\n",
      "local_files_only is False\n",
      "config_file is /home/takuzen/.cache/huggingface/hub/models--lerobot--pi0/snapshots/4329e3639a642c0dd3285e0cf42ba1031c61b5d5/config.json\n",
      "cls is <class 'lerobot.configs.policies.PreTrainedConfig'>\n",
      "cli_overrides is []\n",
      "config_file is /home/takuzen/.cache/huggingface/hub/models--lerobot--pi0/snapshots/4329e3639a642c0dd3285e0cf42ba1031c61b5d5/config.json\n",
      "Loaded config in policies file: PI0Config(n_obs_steps=1, normalization_mapping={'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>, 'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>, 'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>}, input_features={}, output_features={}, chunk_size=50, n_action_steps=50, max_state_dim=32, max_action_dim=32, resize_imgs_with_padding=(224, 224), empty_cameras=0, adapt_to_pi_aloha=False, use_delta_joint_actions_aloha=False, tokenizer_max_length=48, proj_width=1024, num_steps=10, use_cache=True, attention_implementation='eager', freeze_vision_encoder=True, train_expert_only=False, train_state_proj=True, optimizer_betas=(0.9, 0.95), optimizer_eps=1e-08, optimizer_type='sgd', optimizer_lr=2.5e-05, optimizer_momentum=0.9, optimizer_weight_decay=1e-10, scheduler_warmup_steps=1000, scheduler_decay_steps=30000, scheduler_decay_lr=2.5e-06)\n",
      "self.policy is PI0Config(n_obs_steps=1, normalization_mapping={'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>, 'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>, 'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>}, input_features={}, output_features={}, chunk_size=50, n_action_steps=50, max_state_dim=32, max_action_dim=32, resize_imgs_with_padding=(224, 224), empty_cameras=0, adapt_to_pi_aloha=False, use_delta_joint_actions_aloha=False, tokenizer_max_length=48, proj_width=1024, num_steps=10, use_cache=True, attention_implementation='eager', freeze_vision_encoder=True, train_expert_only=False, train_state_proj=True, optimizer_betas=(0.9, 0.95), optimizer_eps=1e-08, optimizer_type='sgd', optimizer_lr=2.5e-05, optimizer_momentum=0.9, optimizer_weight_decay=1e-10, scheduler_warmup_steps=1000, scheduler_decay_steps=30000, scheduler_decay_lr=2.5e-06)\n",
      "self.policy.pretrained_path is lerobot/pi0\n",
      "self.policy.get_optimizer_preset() is SGDConfig(lr=2.5e-05, weight_decay=1e-10, grad_clip_norm=10.0, momentum=0.9, dampening=0.0, nesterov=False)\n",
      "self.policy.get_scheduler_preset() is CosineDecayWithWarmupSchedulerConfig(num_warmup_steps=1000, num_decay_steps=30000, peak_lr=2.5e-05, decay_lr=2.5e-06)\n",
      "after from_pretrained call, cfg is: TrainPipelineConfig(dataset=DatasetConfig(repo_id='lerobot/aloha_sim_transfer_cube_human', episodes=None, image_transforms=ImageTransformsConfig(enable=False, max_num_transforms=3, random_order=False, tfs={'brightness': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'brightness': (0.8, 1.2)}), 'contrast': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'contrast': (0.8, 1.2)}), 'saturation': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'saturation': (0.5, 1.5)}), 'hue': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'hue': (-0.05, 0.05)}), 'sharpness': ImageTransformConfig(weight=1.0, type='SharpnessJitter', kwargs={'sharpness': (0.5, 1.5)})}), local_files_only=False, use_imagenet_stats=True, video_backend='pyav'), env=None, policy=PI0Config(n_obs_steps=1, normalization_mapping={'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>, 'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>, 'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>}, input_features={}, output_features={}, chunk_size=50, n_action_steps=50, max_state_dim=32, max_action_dim=32, resize_imgs_with_padding=(224, 224), empty_cameras=0, adapt_to_pi_aloha=False, use_delta_joint_actions_aloha=False, tokenizer_max_length=48, proj_width=1024, num_steps=10, use_cache=True, attention_implementation='eager', freeze_vision_encoder=True, train_expert_only=False, train_state_proj=True, optimizer_betas=(0.9, 0.95), optimizer_eps=1e-08, optimizer_type='sgd', optimizer_lr=2.5e-05, optimizer_momentum=0.9, optimizer_weight_decay=1e-10, scheduler_warmup_steps=1000, scheduler_decay_steps=30000, scheduler_decay_lr=2.5e-06), output_dir=PosixPath('outputs/train/2025-02-14/05-22-04_pi0'), job_name='pi0', resume=False, device='cuda', use_amp=False, seed=1000, num_workers=4, batch_size=4, eval_freq=20000, log_freq=200, save_checkpoint=True, save_freq=20, offline=OfflineConfig(steps=100000), online=OnlineConfig(steps=0, rollout_n_episodes=1, rollout_batch_size=1, steps_between_rollouts=None, sampling_ratio=0.5, env_seed=None, buffer_capacity=None, buffer_seed_size=0, do_rollout_async=False), use_policy_training_preset=True, optimizer=SGDConfig(lr=2.5e-05, weight_decay=1e-10, grad_clip_norm=10.0, momentum=0.9, dampening=0.0, nesterov=False), scheduler=CosineDecayWithWarmupSchedulerConfig(num_warmup_steps=1000, num_decay_steps=30000, peak_lr=2.5e-05, decay_lr=2.5e-06), eval=EvalConfig(n_episodes=50, batch_size=50, use_async_envs=False), wandb=WandBConfig(enable=False, disable_artifact=False, project='lerobot', entity=None, notes=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 987.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.input_features is {'observation.images.top': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640)), 'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(14,))}\n",
      "cfg.image_features is {'observation.images.top': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640))}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 3947.58it/s]\n",
      "Fetching 106 files: 100%|██████████| 106/106 [00:00<00:00, 1253.62it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before normalize: {'observation.images.top': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), 'observation.state': tensor([[ 0.0000, -0.9600,  1.1600,  0.0000, -0.3000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.9600,  1.1600,  0.0000, -0.3000,  0.0000,  0.0000]]), 'action': tensor([[[-0.0138, -0.9296,  1.1796, -0.0031, -0.3298, -0.0015,  0.1637,\n",
      "           0.0107, -0.9296,  1.2026, -0.0015, -0.3191,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.9127,  1.1796, -0.0046, -0.3313,  0.0000,  0.1637,\n",
      "           0.0107, -0.9265,  1.1996, -0.0015, -0.3175,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.9066,  1.1796, -0.0031, -0.3313,  0.0000,  0.1637,\n",
      "           0.0107, -0.9281,  1.2011, -0.0015, -0.3175,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.9020,  1.1796, -0.0031, -0.3329,  0.0000,  0.1637,\n",
      "           0.0107, -0.9265,  1.2011, -0.0015, -0.3145,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.9004,  1.1812, -0.0031, -0.3359,  0.0015,  0.1637,\n",
      "           0.0107, -0.9265,  1.2026, -0.0015, -0.3145,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.9004,  1.1812, -0.0031, -0.3375,  0.0015,  0.1637,\n",
      "           0.0107, -0.9265,  1.2042, -0.0031, -0.3129,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8989,  1.1812, -0.0046, -0.3390,  0.0015,  0.1637,\n",
      "           0.0092, -0.9265,  1.2057, -0.0015, -0.3114,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8989,  1.1812, -0.0046, -0.3405,  0.0015,  0.1637,\n",
      "           0.0107, -0.9265,  1.2057, -0.0031, -0.3114,  0.0184,  0.1900],\n",
      "         [-0.0138, -0.8989,  1.1827, -0.0031, -0.3421,  0.0031,  0.1637,\n",
      "           0.0107, -0.9250,  1.2057, -0.0015, -0.3114,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.9004,  1.1827, -0.0031, -0.3451,  0.0031,  0.1637,\n",
      "           0.0107, -0.9235,  1.2042, -0.0015, -0.3114,  0.0184,  0.1900],\n",
      "         [-0.0138, -0.9004,  1.1827, -0.0031, -0.3467,  0.0031,  0.1637,\n",
      "           0.0107, -0.9219,  1.2042, -0.0015, -0.3114,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8989,  1.1827, -0.0046, -0.3482,  0.0031,  0.1637,\n",
      "           0.0107, -0.9204,  1.2026, -0.0015, -0.3114,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8989,  1.1827, -0.0046, -0.3497,  0.0031,  0.1637,\n",
      "           0.0107, -0.9189,  1.2026, -0.0015, -0.3114,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8974,  1.1827, -0.0031, -0.3528,  0.0031,  0.1637,\n",
      "           0.0107, -0.9143,  1.2011, -0.0015, -0.3114,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8974,  1.1827, -0.0046, -0.3543,  0.0031,  0.1637,\n",
      "           0.0107, -0.9097,  1.2011, -0.0015, -0.3114,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8958,  1.1842, -0.0031, -0.3574,  0.0046,  0.1637,\n",
      "           0.0107, -0.9050,  1.1996, -0.0015, -0.3129,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.8943,  1.1827, -0.0031, -0.3590,  0.0031,  0.1637,\n",
      "           0.0107, -0.8974,  1.1996, -0.0015, -0.3175,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.8928,  1.1842, -0.0031, -0.3636,  0.0031,  0.1637,\n",
      "           0.0107, -0.8897,  1.1965, -0.0015, -0.3191,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.8897,  1.1842, -0.0031, -0.3651,  0.0031,  0.1637,\n",
      "           0.0107, -0.8805,  1.1934, -0.0031, -0.3191,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.8866,  1.1858, -0.0031, -0.3682,  0.0031,  0.1637,\n",
      "           0.0092, -0.8698,  1.1904, -0.0015, -0.3206,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.8820,  1.1873, -0.0046, -0.3712,  0.0031,  0.1637,\n",
      "           0.0107, -0.8560,  1.1858, -0.0015, -0.3191,  0.0153,  0.1900],\n",
      "         [-0.0138, -0.8790,  1.1873, -0.0046, -0.3743,  0.0031,  0.1637,\n",
      "           0.0107, -0.8406,  1.1827, -0.0015, -0.3191,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8744,  1.1873, -0.0031, -0.3774,  0.0031,  0.1637,\n",
      "           0.0107, -0.8237,  1.1812,  0.0000, -0.3175,  0.0169,  0.1900],\n",
      "         [-0.0138, -0.8698,  1.1873, -0.0046, -0.3820,  0.0031,  0.1637,\n",
      "           0.0107, -0.8069,  1.1781,  0.0000, -0.3145,  0.0184,  0.1908],\n",
      "         [-0.0138, -0.8652,  1.1888, -0.0046, -0.3850,  0.0031,  0.1637,\n",
      "           0.0107, -0.7885,  1.1796,  0.0015, -0.3114,  0.0199,  0.1908],\n",
      "         [-0.0138, -0.8606,  1.1888, -0.0046, -0.3881,  0.0031,  0.1637,\n",
      "           0.0107, -0.7731,  1.1812,  0.0046, -0.3083,  0.0199,  0.1908],\n",
      "         [-0.0138, -0.8560,  1.1888, -0.0031, -0.3896,  0.0031,  0.1637,\n",
      "           0.0107, -0.7578,  1.1812,  0.0077, -0.3022,  0.0199,  0.1908],\n",
      "         [-0.0138, -0.8514,  1.1873, -0.0046, -0.3927,  0.0031,  0.1637,\n",
      "           0.0107, -0.7440,  1.1827,  0.0138, -0.2915,  0.0215,  0.1908],\n",
      "         [-0.0138, -0.8483,  1.1888, -0.0046, -0.3942,  0.0031,  0.1637,\n",
      "           0.0092, -0.7286,  1.1812,  0.0215, -0.2792,  0.0230,  0.1916],\n",
      "         [-0.0138, -0.8437,  1.1888, -0.0046, -0.3958,  0.0031,  0.1637,\n",
      "           0.0077, -0.7148,  1.1781,  0.0261, -0.2654,  0.0261,  0.1916],\n",
      "         [-0.0138, -0.8406,  1.1888, -0.0031, -0.3973,  0.0031,  0.1637,\n",
      "           0.0046, -0.7026,  1.1750,  0.0291, -0.2500,  0.0276,  0.1916],\n",
      "         [-0.0138, -0.8360,  1.1888, -0.0046, -0.4004,  0.0031,  0.1637,\n",
      "          -0.0031, -0.6918,  1.1735,  0.0291, -0.2332,  0.0322,  0.1916],\n",
      "         [-0.0138, -0.8330,  1.1873, -0.0046, -0.4034,  0.0031,  0.1637,\n",
      "          -0.0107, -0.6811,  1.1704,  0.0307, -0.2178,  0.0353,  0.1916],\n",
      "         [-0.0138, -0.8299,  1.1888, -0.0046, -0.4065,  0.0031,  0.1637,\n",
      "          -0.0184, -0.6719,  1.1689,  0.0307, -0.2025,  0.0368,  0.1916],\n",
      "         [-0.0138, -0.8268,  1.1888, -0.0031, -0.4080,  0.0031,  0.1637,\n",
      "          -0.0261, -0.6642,  1.1689,  0.0291, -0.1917,  0.0399,  0.1916],\n",
      "         [-0.0138, -0.8237,  1.1873, -0.0046, -0.4096,  0.0031,  0.1637,\n",
      "          -0.0322, -0.6565,  1.1704,  0.0307, -0.1856,  0.0399,  0.1923],\n",
      "         [-0.0138, -0.8222,  1.1888, -0.0046, -0.4096,  0.0031,  0.1637,\n",
      "          -0.0399, -0.6504,  1.1704,  0.0291, -0.1795,  0.0383,  0.1923],\n",
      "         [-0.0138, -0.8207,  1.1888, -0.0046, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0445, -0.6458,  1.1704,  0.0291, -0.1779,  0.0368,  0.1939],\n",
      "         [-0.0138, -0.8176,  1.1873, -0.0061, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0522, -0.6412,  1.1720,  0.0276, -0.1779,  0.0337,  0.1971],\n",
      "         [-0.0138, -0.8161,  1.1888, -0.0061, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0552, -0.6351,  1.1735,  0.0261, -0.1795,  0.0322,  0.2020],\n",
      "         [-0.0138, -0.8145,  1.1888, -0.0046, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0598, -0.6305,  1.1750,  0.0261, -0.1779,  0.0307,  0.2094],\n",
      "         [-0.0138, -0.8130,  1.1888, -0.0061, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0644, -0.6243,  1.1781,  0.0245, -0.1779,  0.0307,  0.2186],\n",
      "         [-0.0138, -0.8115,  1.1888, -0.0046, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0675, -0.6197,  1.1796,  0.0215, -0.1795,  0.0291,  0.2299],\n",
      "         [-0.0138, -0.8099,  1.1888, -0.0061, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0706, -0.6136,  1.1812,  0.0199, -0.1810,  0.0291,  0.2442],\n",
      "         [-0.0138, -0.8099,  1.1888, -0.0061, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0706, -0.6075,  1.1842,  0.0184, -0.1825,  0.0291,  0.2628],\n",
      "         [-0.0138, -0.8084,  1.1888, -0.0061, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0706, -0.6013,  1.1858,  0.0153, -0.1871,  0.0276,  0.2842],\n",
      "         [-0.0138, -0.8069,  1.1888, -0.0061, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0706, -0.5967,  1.1873,  0.0153, -0.1917,  0.0276,  0.3099],\n",
      "         [-0.0138, -0.8069,  1.1888, -0.0061, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0706, -0.5890,  1.1888,  0.0138, -0.1979,  0.0276,  0.3413],\n",
      "         [-0.0138, -0.8053,  1.1888, -0.0046, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0706, -0.5814,  1.1904,  0.0123, -0.2056,  0.0261,  0.3890],\n",
      "         [-0.0138, -0.8053,  1.1904, -0.0046, -0.4096,  0.0015,  0.1637,\n",
      "          -0.0706, -0.5722,  1.1919,  0.0123, -0.2132,  0.0276,  0.4328]]]), 'episode_index': tensor([0]), 'frame_index': tensor([0]), 'timestamp': tensor([0.]), 'next.done': tensor([False]), 'index': tensor([0]), 'task_index': tensor([0]), 'action_is_pad': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]]), 'task': ['Pick up the cube with the right arm and transfer it to the left arm.']}\n",
      "after normalize: {'observation.images.top': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), 'observation.state': tensor([[ 0.6073, -1.1028,  0.7433,  1.0620,  1.0901,  0.8497, -1.4950,  0.3444,\n",
      "         -2.2481,  1.4155,  0.8292,  0.0332,  0.4432, -2.6828]]), 'action': tensor([[[-4.9820e-01, -1.0192e+00,  8.8823e-01,  9.5051e-01,  9.5937e-01,\n",
      "           8.3299e-01, -6.8553e-01,  4.5213e-01, -2.1887e+00,  1.6775e+00,\n",
      "           8.0663e-01, -2.1624e-03,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.6212e-01,  8.8823e-01,  9.1705e-01,  9.4903e-01,\n",
      "           8.5049e-01, -6.8553e-01,  4.5213e-01, -2.1776e+00,  1.6605e+00,\n",
      "           8.0663e-01,  2.8330e-03,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.4137e-01,  8.8823e-01,  9.5051e-01,  9.4903e-01,\n",
      "           8.5049e-01, -6.8553e-01,  4.5213e-01, -2.1832e+00,  1.6690e+00,\n",
      "           8.0663e-01,  2.8330e-03,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.2581e-01,  8.8823e-01,  9.5051e-01,  9.3869e-01,\n",
      "           8.5049e-01, -6.8553e-01,  4.5213e-01, -2.1776e+00,  1.6690e+00,\n",
      "           8.0663e-01,  1.2824e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.2062e-01,  8.9741e-01,  9.5051e-01,  9.1800e-01,\n",
      "           8.6800e-01, -6.8553e-01,  4.5213e-01, -2.1776e+00,  1.6775e+00,\n",
      "           8.0663e-01,  1.2824e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.2062e-01,  8.9741e-01,  9.5051e-01,  9.0766e-01,\n",
      "           8.6800e-01, -6.8553e-01,  4.5213e-01, -2.1776e+00,  1.6860e+00,\n",
      "           8.0080e-01,  1.7819e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.1544e-01,  8.9741e-01,  9.1705e-01,  8.9732e-01,\n",
      "           8.6800e-01, -6.8553e-01,  4.3766e-01, -2.1776e+00,  1.6945e+00,\n",
      "           8.0663e-01,  2.2815e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.1544e-01,  8.9741e-01,  9.1705e-01,  8.8698e-01,\n",
      "           8.6800e-01, -6.8553e-01,  4.5213e-01, -2.1776e+00,  1.6945e+00,\n",
      "           8.0080e-01,  2.2815e-02,  4.7503e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.1544e-01,  9.0660e-01,  9.5051e-01,  8.7664e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.1720e+00,  1.6945e+00,\n",
      "           8.0663e-01,  2.2815e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.2062e-01,  9.0660e-01,  9.5051e-01,  8.5596e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.1665e+00,  1.6860e+00,\n",
      "           8.0663e-01,  2.2815e-02,  4.7503e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.2062e-01,  9.0660e-01,  9.5051e-01,  8.4562e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.1609e+00,  1.6860e+00,\n",
      "           8.0663e-01,  2.2815e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.1544e-01,  9.0660e-01,  9.1705e-01,  8.3527e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.1554e+00,  1.6775e+00,\n",
      "           8.0663e-01,  2.2815e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.1544e-01,  9.0660e-01,  9.1705e-01,  8.2493e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.1498e+00,  1.6775e+00,\n",
      "           8.0663e-01,  2.2815e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.1025e-01,  9.0660e-01,  9.5051e-01,  8.0425e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.1331e+00,  1.6690e+00,\n",
      "           8.0663e-01,  2.2815e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.1025e-01,  9.0660e-01,  9.1705e-01,  7.9391e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.1164e+00,  1.6690e+00,\n",
      "           8.0663e-01,  2.2815e-02,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -9.0506e-01,  9.1578e-01,  9.5051e-01,  7.7323e-01,\n",
      "           9.0301e-01, -6.8553e-01,  4.5213e-01, -2.0997e+00,  1.6605e+00,\n",
      "           8.0663e-01,  1.7819e-02,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -8.9988e-01,  9.0660e-01,  9.5051e-01,  7.6288e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.0719e+00,  1.6605e+00,\n",
      "           8.0663e-01,  2.8330e-03,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -8.9469e-01,  9.1578e-01,  9.5051e-01,  7.3186e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.0441e+00,  1.6435e+00,\n",
      "           8.0663e-01, -2.1624e-03,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -8.8432e-01,  9.1578e-01,  9.5051e-01,  7.2152e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -2.0107e+00,  1.6265e+00,\n",
      "           8.0080e-01, -2.1624e-03,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -8.7394e-01,  9.2497e-01,  9.5051e-01,  7.0084e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.3766e-01, -1.9718e+00,  1.6095e+00,\n",
      "           8.0663e-01, -7.1578e-03,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -8.5838e-01,  9.3415e-01,  9.1705e-01,  6.8015e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -1.9217e+00,  1.5840e+00,\n",
      "           8.0663e-01, -2.1624e-03,  4.6925e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -8.4801e-01,  9.3415e-01,  9.1705e-01,  6.5947e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -1.8661e+00,  1.5670e+00,\n",
      "           8.0663e-01, -2.1624e-03,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -8.3245e-01,  9.3415e-01,  9.5051e-01,  6.3879e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -1.8049e+00,  1.5586e+00,\n",
      "           8.1246e-01,  2.8330e-03,  4.7214e-01, -9.4311e-01],\n",
      "         [-4.9820e-01, -8.1688e-01,  9.3415e-01,  9.1705e-01,  6.0776e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -1.7437e+00,  1.5416e+00,\n",
      "           8.1246e-01,  1.2824e-02,  4.7503e-01, -9.4106e-01],\n",
      "         [-4.9820e-01, -8.0132e-01,  9.4334e-01,  9.1705e-01,  5.8708e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -1.6769e+00,  1.5501e+00,\n",
      "           8.1830e-01,  2.2815e-02,  4.7792e-01, -9.4106e-01],\n",
      "         [-4.9820e-01, -7.8576e-01,  9.4334e-01,  9.1705e-01,  5.6640e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -1.6213e+00,  1.5586e+00,\n",
      "           8.2996e-01,  3.2806e-02,  4.7792e-01, -9.4106e-01],\n",
      "         [-4.9820e-01, -7.7020e-01,  9.4334e-01,  9.5051e-01,  5.5606e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -1.5656e+00,  1.5586e+00,\n",
      "           8.4162e-01,  5.2787e-02,  4.7792e-01, -9.4106e-01],\n",
      "         [-4.9820e-01, -7.5464e-01,  9.3415e-01,  9.1705e-01,  5.3537e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.5213e-01, -1.5155e+00,  1.5670e+00,\n",
      "           8.6495e-01,  8.7755e-02,  4.8081e-01, -9.4106e-01],\n",
      "         [-4.9820e-01, -7.4427e-01,  9.4334e-01,  9.1705e-01,  5.2503e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.3766e-01, -1.4599e+00,  1.5586e+00,\n",
      "           8.9411e-01,  1.2772e-01,  4.8371e-01, -9.3901e-01],\n",
      "         [-4.9820e-01, -7.2871e-01,  9.4334e-01,  9.1705e-01,  5.1469e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.2319e-01, -1.4098e+00,  1.5416e+00,\n",
      "           9.1160e-01,  1.7268e-01,  4.8949e-01, -9.3901e-01],\n",
      "         [-4.9820e-01, -7.1833e-01,  9.4334e-01,  9.5051e-01,  5.0435e-01,\n",
      "           8.8550e-01, -6.8553e-01,  3.9425e-01, -1.3653e+00,  1.5246e+00,\n",
      "           9.2327e-01,  2.2263e-01,  4.9238e-01, -9.3901e-01],\n",
      "         [-4.9820e-01, -7.0277e-01,  9.4334e-01,  9.1705e-01,  4.8367e-01,\n",
      "           8.8550e-01, -6.8553e-01,  3.2189e-01, -1.3264e+00,  1.5161e+00,\n",
      "           9.2327e-01,  2.7758e-01,  5.0105e-01, -9.3901e-01],\n",
      "         [-4.9820e-01, -6.9240e-01,  9.3415e-01,  9.1705e-01,  4.6299e-01,\n",
      "           8.8550e-01, -6.8553e-01,  2.4953e-01, -1.2874e+00,  1.4991e+00,\n",
      "           9.2910e-01,  3.2754e-01,  5.0684e-01, -9.3901e-01],\n",
      "         [-4.9820e-01, -6.8202e-01,  9.4334e-01,  9.1705e-01,  4.4230e-01,\n",
      "           8.8550e-01, -6.8553e-01,  1.7717e-01, -1.2541e+00,  1.4906e+00,\n",
      "           9.2910e-01,  3.7749e-01,  5.0973e-01, -9.3901e-01],\n",
      "         [-4.9820e-01, -6.7165e-01,  9.4334e-01,  9.5051e-01,  4.3196e-01,\n",
      "           8.8550e-01, -6.8553e-01,  1.0481e-01, -1.2262e+00,  1.4906e+00,\n",
      "           9.2327e-01,  4.1246e-01,  5.1551e-01, -9.3901e-01],\n",
      "         [-4.9820e-01, -6.6128e-01,  9.3415e-01,  9.1705e-01,  4.2162e-01,\n",
      "           8.8550e-01, -6.8553e-01,  4.6925e-02, -1.1984e+00,  1.4991e+00,\n",
      "           9.2910e-01,  4.3244e-01,  5.1551e-01, -9.3695e-01],\n",
      "         [-4.9820e-01, -6.5609e-01,  9.4334e-01,  9.1705e-01,  4.2162e-01,\n",
      "           8.8550e-01, -6.8553e-01, -2.5434e-02, -1.1762e+00,  1.4991e+00,\n",
      "           9.2327e-01,  4.5242e-01,  5.1262e-01, -9.3695e-01],\n",
      "         [-4.9820e-01, -6.5090e-01,  9.4334e-01,  9.1705e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -6.8849e-02, -1.1595e+00,  1.4991e+00,\n",
      "           9.2327e-01,  4.5742e-01,  5.0973e-01, -9.3283e-01],\n",
      "         [-4.9820e-01, -6.4053e-01,  9.3415e-01,  8.8359e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -1.4121e-01, -1.1428e+00,  1.5076e+00,\n",
      "           9.1744e-01,  4.5742e-01,  5.0395e-01, -9.2452e-01],\n",
      "         [-4.9820e-01, -6.3534e-01,  9.4334e-01,  8.8359e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -1.7015e-01, -1.1205e+00,  1.5161e+00,\n",
      "           9.1160e-01,  4.5242e-01,  5.0105e-01, -9.1190e-01],\n",
      "         [-4.9820e-01, -6.3015e-01,  9.4334e-01,  9.1705e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -2.1357e-01, -1.1038e+00,  1.5246e+00,\n",
      "           9.1160e-01,  4.5742e-01,  4.9816e-01, -8.9263e-01],\n",
      "         [-4.9820e-01, -6.2497e-01,  9.4334e-01,  8.8359e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -2.5698e-01, -1.0816e+00,  1.5416e+00,\n",
      "           9.0577e-01,  4.5742e-01,  4.9816e-01, -8.6852e-01],\n",
      "         [-4.9820e-01, -6.1978e-01,  9.4334e-01,  9.1705e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -2.8593e-01, -1.0649e+00,  1.5501e+00,\n",
      "           8.9411e-01,  4.5242e-01,  4.9527e-01, -8.3922e-01],\n",
      "         [-4.9820e-01, -6.1459e-01,  9.4334e-01,  8.8359e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -3.1487e-01, -1.0426e+00,  1.5586e+00,\n",
      "           8.8828e-01,  4.4743e-01,  4.9527e-01, -8.0194e-01],\n",
      "         [-4.9820e-01, -6.1459e-01,  9.4334e-01,  8.8359e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -3.1487e-01, -1.0204e+00,  1.5755e+00,\n",
      "           8.8244e-01,  4.4243e-01,  4.9527e-01, -7.5343e-01],\n",
      "         [-4.9820e-01, -6.0941e-01,  9.4334e-01,  8.8359e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -3.1487e-01, -9.9814e-01,  1.5840e+00,\n",
      "           8.7078e-01,  4.2744e-01,  4.9238e-01, -6.9753e-01],\n",
      "         [-4.9820e-01, -6.0422e-01,  9.4334e-01,  8.8359e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -3.1487e-01, -9.8145e-01,  1.5925e+00,\n",
      "           8.7078e-01,  4.1246e-01,  4.9238e-01, -6.3074e-01],\n",
      "         [-4.9820e-01, -6.0422e-01,  9.4334e-01,  8.8359e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -3.1487e-01, -9.5363e-01,  1.6010e+00,\n",
      "           8.6495e-01,  3.9248e-01,  4.9238e-01, -5.4878e-01],\n",
      "         [-4.9820e-01, -5.9903e-01,  9.4334e-01,  9.1705e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -3.1487e-01, -9.2581e-01,  1.6095e+00,\n",
      "           8.5912e-01,  3.6750e-01,  4.8949e-01, -4.2447e-01],\n",
      "         [-4.9820e-01, -5.9903e-01,  9.5252e-01,  9.1705e-01,  4.2162e-01,\n",
      "           8.6800e-01, -6.8553e-01, -3.1487e-01, -8.9243e-01,  1.6180e+00,\n",
      "           8.5912e-01,  3.4252e-01,  4.9238e-01, -3.1035e-01]]]), 'episode_index': tensor([0]), 'frame_index': tensor([0]), 'timestamp': tensor([0.]), 'next.done': tensor([False]), 'index': tensor([0]), 'task_index': tensor([0]), 'action_is_pad': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]]), 'task': ['Pick up the cube with the right arm and transfer it to the left arm.']}\n",
      "processing img key: observation.images.top\n",
      "img shape: torch.Size([1, 3, 224, 224])\n",
      "img: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "img: tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]]])\n",
      "['Pick up the cube with the right arm and transfer it to the left arm.']\n",
      "['Pick up the cube with the right arm and transfer it to the left arm.\\n']\n",
      "48\n",
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 50, 32])\n",
      "torch.Size([1, 50, 32])\n",
      "img_emb torch.Size([1, 256, 2048])\n",
      "img_mask torch.Size([1])\n",
      "img_mask torch.Size([1, 256])\n",
      "lang_emb torch.Size([1, 48, 2048])\n",
      "lang_emb_dim 2048\n",
      "lang_masks torch.Size([1, 48])\n",
      "state_emb torch.Size([1, 1024])\n",
      "action_emb torch.Size([1, 50, 1024])\n",
      "time_emb torch.Size([1, 1024])\n",
      "action_time_emb torch.Size([1, 50, 2048])\n",
      "embs len 2\n",
      "Embedding 0 shape: torch.Size([1, 1, 1024])\n",
      "Embedding 1 shape: torch.Size([1, 50, 1024])\n",
      "action_mask len 2\n",
      "action_mask 0 shape: torch.Size([1, 1])\n",
      "action_mask 1 shape: torch.Size([1, 50])\n",
      "att_masks shape torch.Size([51])\n",
      "prefix_embs torch.Size([1, 304, 2048])\n",
      "prefix_pad_masks torch.Size([1, 304])\n",
      "prefix_att_masks torch.Size([1, 304])\n",
      "suffix_embs torch.Size([1, 51, 1024])\n",
      "suffix_pad_masks torch.Size([1, 51])\n",
      "suffix_att_masks torch.Size([1, 51])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "hideen_states.shape: torch.Size([1, 304, 2048])\n",
      "query_state.shape: torch.Size([1, 304, 8, 256])\n",
      "key_state.shape: torch.Size([1, 304, 1, 256])\n",
      "value_state.shape: torch.Size([1, 304, 1, 256])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "hideen_states.shape: torch.Size([1, 51, 1024])\n",
      "query_state.shape: torch.Size([1, 51, 8, 256])\n",
      "key_state.shape: torch.Size([1, 51, 1, 256])\n",
      "value_state.shape: torch.Size([1, 51, 1, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 8, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 8, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "after rope query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "apply_rope\n",
      "query states shape: torch.Size([1, 355, 1, 256])\n",
      "d_half: 128\n",
      "x shape: torch.Size([1, 355, 1, 256])\n",
      "freq_exponents shape: torch.Size([128])\n",
      "timescale shape: torch.Size([128])\n",
      "positions shape: torch.Size([1, 355])\n",
      "positions[..., None] shape: torch.Size([1, 355, 1])\n",
      "timescale[None, None, :] shape: torch.Size([1, 1, 128])\n",
      "radians shape: torch.Size([1, 355, 128])\n",
      "num_attn_heads: 8\n",
      "num_key_value_heads: 1\n",
      "num_key_value_groups: 8\n",
      "sequence_length: 355\n",
      "value_states.shape: torch.Size([1, 355, 1, 256])\n",
      "key_states.shape: torch.Size([1, 355, 8, 256])\n",
      "query_states.shape: torch.Size([1, 355, 8, 256])\n",
      "att_weights.shape: torch.Size([1, 8, 355, 355])\n",
      "probs.shape: torch.Size([1, 8, 355, 355])\n",
      "value_states.shape: torch.Size([1, 355, 8, 256])\n",
      "attn_output.shape: torch.Size([1, 8, 355, 256])\n",
      "attn_output.shape: torch.Size([1, 355, 2048])\n",
      "att_output after qkv: torch.Size([1, 355, 2048])\n",
      "Output Embedding 0 shape: torch.Size([51, 1024])\n",
      "suffix_out shape: torch.Size([1, 50, 1024])\n",
      "v_t shape: torch.Size([1, 50, 32])\n",
      "losses: 1.734375\n",
      "response is None\n"
     ]
    }
   ],
   "source": [
    "@parser.wrap()\n",
    "def train(cfg: TrainPipelineConfig):\n",
    "    print(\"Before from_pretrained call, cfg is:\", cfg)\n",
    "    cfg.validate()\n",
    "    print(\"after from_pretrained call, cfg is:\", cfg)\n",
    "\n",
    "    image_transforms = (\n",
    "        ImageTransforms(cfg.dataset.image_transforms) if cfg.dataset.image_transforms.enable else None\n",
    "    )\n",
    "\n",
    "    ds_meta = LeRobotDatasetMetadata(cfg.dataset.repo_id, local_files_only=cfg.dataset.local_files_only)\n",
    "\n",
    "    features = dataset_to_policy_features(ds_meta.features)\n",
    "\n",
    "    cfg.output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "    cfg.input_features = {key: ft for key, ft in features.items() if key not in cfg.output_features}\n",
    "    print(f'cfg.input_features is {cfg.input_features}')\n",
    "    delta_timestamps = resolve_delta_timestamps(cfg.policy, ds_meta)\n",
    "\n",
    "    cfg.image_features = {key: ft for key, ft in cfg.input_features.items() if ft.type is FeatureType.VISUAL}\n",
    "    print(f'cfg.image_features is {cfg.image_features}')\n",
    "\n",
    "    dataset = LeRobotDataset(\n",
    "        cfg.dataset.repo_id,\n",
    "        episodes=cfg.dataset.episodes,\n",
    "        delta_timestamps=delta_timestamps,\n",
    "        image_transforms=image_transforms,\n",
    "        video_backend=cfg.dataset.video_backend,\n",
    "        local_files_only=cfg.dataset.local_files_only,\n",
    "    )\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        num_workers=4,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=\"gpu\",\n",
    "    )\n",
    "    dl_iter = cycle(dataloader)\n",
    "    \n",
    "    model = ModelRunner(cfg, ds_meta)\n",
    "    for _ in range(1):\n",
    "        batch = next(dl_iter)\n",
    "        output = model(batch)\n",
    "        # print(output)\n",
    "\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after from_pretrained call, cfg is: TrainPipelineConfig(dataset=DatasetConfig(repo_id='lerobot/aloha_sim_transfer_cube_human', episodes=None, image_transforms=ImageTransformsConfig(enable=False, max_num_transforms=3, random_order=False, tfs={'brightness': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'brightness': (0.8, 1.2)}), 'contrast': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'contrast': (0.8, 1.2)}), 'saturation': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'saturation': (0.5, 1.5)}), 'hue': ImageTransformConfig(weight=1.0, type='ColorJitter', kwargs={'hue': (-0.05, 0.05)}), 'sharpness': ImageTransformConfig(weight=1.0, type='SharpnessJitter', kwargs={'sharpness': (0.5, 1.5)})}), local_files_only=False, use_imagenet_stats=True, video_backend='pyav'), env=None, policy=PI0Config(n_obs_steps=1, normalization_mapping={'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>, 'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>, 'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>}, input_features={}, output_features={}, chunk_size=50, n_action_steps=50, max_state_dim=32, max_action_dim=32, resize_imgs_with_padding=(224, 224), empty_cameras=0, adapt_to_pi_aloha=False, use_delta_joint_actions_aloha=False, tokenizer_max_length=48, proj_width=1024, num_steps=10, use_cache=True, attention_implementation='eager', freeze_vision_encoder=True, train_expert_only=False, train_state_proj=True, optimizer_betas=(0.9, 0.95), optimizer_eps=1e-08, optimizer_type='sgd', optimizer_lr=2.5e-05, optimizer_momentum=0.9, optimizer_weight_decay=1e-10, scheduler_warmup_steps=1000, scheduler_decay_steps=30000, scheduler_decay_lr=2.5e-06), output_dir=PosixPath('outputs/train/2025-02-10/23-39-54_pi0'), job_name='pi0', resume=False, device='cuda', use_amp=False, seed=1000, num_workers=4, batch_size=4, eval_freq=20000, log_freq=200, save_checkpoint=True, save_freq=20, offline=OfflineConfig(steps=100000), online=OnlineConfig(steps=0, rollout_n_episodes=1, rollout_batch_size=1, steps_between_rollouts=None, sampling_ratio=0.5, env_seed=None, buffer_capacity=None, buffer_seed_size=0, do_rollout_async=False), use_policy_training_preset=True, optimizer=SGDConfig(lr=2.5e-05, weight_decay=1e-10, grad_clip_norm=10.0, momentum=0.9, dampening=0.0, nesterov=False), scheduler=CosineDecayWithWarmupSchedulerConfig(num_warmup_steps=1000, num_decay_steps=30000, peak_lr=2.5e-05, decay_lr=2.5e-06), eval=EvalConfig(n_episodes=50, batch_size=50, use_async_envs=False), wandb=WandBConfig(enable=False, disable_artifact=False, project='lerobot', entity=None, notes=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = draccus.parse(PreTrainedConfig, config_file, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
